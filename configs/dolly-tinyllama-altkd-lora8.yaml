# ModelArguments
model_name_or_path: ../models/tinyllama/tiny_llama_2_5T/
torch_dtype: float16
tokenizer_name: ../models/tinyllama/tiny_llama_2_5T/

tokenizer_kwargs:
  use_fast: False
  padding_side: right
  
# DataArguments
dataset_name: dolly

# Seq2SeqTrainingArguments
output_dir: tinyllama-altkd-lora8
num_train_epochs: 10
learning_rate: 0.0001  # 5e-5
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
per_device_eval_batch_size: 4
fp16: true
adam_epsilon: 0.001 # 1e-3
# lr_scheduler_type: cosine
weight_decay: 0.01  # 1e-2
optim: adamw_torch
save_strategy: steps
evaluation_strategy: steps
logging_strategy: steps
eval_steps: 0.5
save_steps: 0.5
logging_steps: 0.05
save_total_limit: 50
# load_best_model_at_end: True
report_to: tensorboard
# gradient_checkpointing=True
ddp_find_unused_parameters: true
remove_unused_columns: False
deepspeed: ds_config/ds_config_zero1.json

# KDArguments
kd_type: altkd
tensor_parallel: false
kd_args:
  reverse_kld: true
  kd_ratio: 0.5
  kd_temperature: 1.0
  altkd_switch_freq: 1
  altkd_adapter_config:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    target_modules:   # all
      - q_proj
      - k_proj
      - v_proj