{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-28 09:44:22,747] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import deepspeed\n",
    "import vllm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import GPUtil\n",
    "import pandas as pd\n",
    "\n",
    "def gpu_stat():\n",
    "    print(\"  \".join([f\"{i}: {gpu.memoryUtil*100:.2f}%\" for i, gpu in enumerate(GPUtil.getGPUs())]))\n",
    "    # pprint([\": \".join([str(i), str(round(gpu.memoryUtil * 100, 2))]) for i,gpu in enumerate(GPUtil.getGPUs())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"4,5,6,7\"\n",
    "os.environ['WORLD_SIZE'] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data/samsum\")\n",
    "MODEL_PATH = Path(\"../models/gpt2/base/\")\n",
    "WORK_DIR = Path('results/samsum/gpt2-base-kd')\n",
    "TEACHER_MODEL_PATH = Path(\"./results/samsum/gpt2-xlarge-sft/checkpoint-4600/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-27 22:12:33.334\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[34m\u001b[1mDatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2023-11-27 22:12:33.335\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[34m\u001b[1mTrain data example:\n",
      "[INST] <<SYS>>\n",
      "Use the Input to provide a summary of a conversation.\n",
      "<</SYS>>\n",
      "\n",
      "Input:\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "Summary:\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\u001b[0m\n",
      "\u001b[32m2023-11-27 22:12:33.392\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mThe padding token id is 50256\u001b[0m\n",
      "Loading cached processed dataset at /home/yzhangjy/LLM/llm-kd/data/samsum/train/cache-a7c3e0fbe7fd8baf.arrow\n",
      "Loading cached processed dataset at /home/yzhangjy/LLM/llm-kd/data/samsum/test/cache-abc99519cc2c886d.arrow\n",
      "Loading cached processed dataset at /home/yzhangjy/LLM/llm-kd/data/samsum/validation/cache-b55bb788d4c9445b.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(str(DATA_PATH))\n",
    "logger.debug(dataset)\n",
    "\n",
    "prompt_template = \"\"\"[INST] <<SYS>>\n",
    "Use the Input to provide a summary of a conversation.\n",
    "<</SYS>>\n",
    "\n",
    "Input:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\"\"\"\n",
    "\n",
    "logger.debug(\"Train data example:\\n\" + prompt_template.format(**dataset['train'][0]))\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "logger.debug(f\"The padding token id is {tokenizer.pad_token_id}\")\n",
    "\n",
    "CUTOFF_LEN = 256\n",
    "LABEL_SPLIT = \"Summary:\\n\"\n",
    "\n",
    "def generate_and_tokenize_prompt(instance, is_test=False):\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=CUTOFF_LEN,\n",
    "            padding=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        if(\n",
    "            result['input_ids'][-1] != tokenizer.eos_token_id\n",
    "            and len(result['input_ids']) < CUTOFF_LEN\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result['input_ids'].append(tokenizer.eos_token_id)\n",
    "            result['attention_mask'].append(1)\n",
    "        result['labels'] = result['input_ids'].copy()\n",
    "        return result\n",
    "    tokenized_full_prompt = tokenize(prompt_template.format(**instance))\n",
    "    tokenized_user_prompt = tokenize(prompt_template.format(**instance).split(LABEL_SPLIT)[0] + LABEL_SPLIT, add_eos_token=False)\n",
    "    user_prompt_len = len(tokenized_user_prompt['input_ids'])\n",
    "    tokenized_full_prompt['labels'] = [-100]*user_prompt_len + tokenized_full_prompt['labels'][user_prompt_len:]\n",
    "    if is_test:\n",
    "        tokenized_user_prompt['_id'] = instance['id']\n",
    "        return tokenized_user_prompt\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "tokenized_dataset = dataset.map(generate_and_tokenize_prompt, num_proc=1) \\\n",
    "                           .remove_columns(dataset['train'].column_names) \\\n",
    "                           .with_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.83%  1: 21.73%  2: 0.03%  3: 0.03%  4: 0.03%  5: 0.03%  6: 0.03%  7: 33.04%\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda(7)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0     1    2    3    4    5    6    7\n",
      "mems   6.6  25.7  0.0  0.0  0.0  0.0  0.0  4.4\n",
      "loads  0.0   0.0  0.0  0.0  0.0  0.0  0.0  6.0\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = AutoModelForCausalLM.from_pretrained(TEACHER_MODEL_PATH, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.cuda(7)\n",
    "teacher_model.eval()\n",
    "teacher_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0     1    2    3    4    5    6     7\n",
      "mems   6.6  25.7  0.0  0.0  0.0  0.0  0.0  17.0\n",
      "loads  0.0   0.0  0.0  0.0  0.0  0.0  0.0  10.0\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    return_tensors=\"pt\",\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# %%\n",
    "dataloader = DataLoader(tokenized_dataset['train'], collate_fn=data_collator, batch_size=8, pin_memory=True, pin_memory_device=\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "data = next(dataloader._get_iterator())\n",
    "\n",
    "for k, v in data.items():\n",
    "    data[k] = v.cuda(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6      7\n",
      "mems   6.56  25.66  0.03  0.03  0.03  0.03  0.03  16.99\n",
      "loads  0.00   0.00  0.00  0.00  0.00  0.00  0.00   0.00\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6      7\n",
      "mems   6.56  25.66  0.03  0.03  0.03  0.03  0.03  26.23\n",
      "loads  0.00   0.00  0.00  0.00  0.00  0.00  0.00   5.00\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_teacher = teacher_model.forward(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6      7\n",
      "mems   6.56  25.66  0.03  0.03  0.03  0.03  0.03  33.29\n",
      "loads  0.00   0.00  0.00  0.00  0.00  0.00  0.00   0.00\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6      7\n",
      "mems   6.56  13.11  0.03  0.03  0.03  0.03  0.03  31.58\n",
      "loads  1.00   0.00  0.00  0.00  0.00  0.00  0.00   4.00\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf_mask = torch.isinf(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(output.logits/5, dim=-1, dtype=torch.float32)\n",
    "probs_teacher = F.softmax(output_teacher.logits/5, dim=-1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6     7\n",
      "mems   9.83  17.72  0.03  0.03  0.03  0.03  0.03  35.6\n",
      "loads  0.00   0.00  0.00  0.00  0.00  0.00  0.00   0.0\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/lmflow/lib/python3.11/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_kd = F.kl_div(probs.log(), probs_teacher, log_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6      7\n",
      "mems   9.83  17.72  0.03  0.03  0.03  0.03  0.03  42.01\n",
      "loads  0.00   0.00  0.00  0.00  0.00  0.00  0.00   0.00\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8731e-06, device='cuda:7', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_kd.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6      7\n",
      "mems   9.83  17.72  0.03  0.03  0.03  0.03  0.03  39.64\n",
      "loads  0.00   0.00  0.00  0.00  0.00  0.00  0.00   0.00\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6      7\n",
      "mems   9.83  17.72  0.03  0.03  0.03  0.03  0.03  39.64\n",
      "loads  0.00   0.00  0.00  0.00  0.00  0.00  0.00   0.00\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1     2     3     4     5     6      7\n",
      "mems   9.83  17.72  0.03  0.03  0.03  0.03  0.03  39.64\n",
      "loads  0.00   0.00  0.00  0.00  0.00  0.00  0.00   0.00\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 9.83),\n",
      " (1, 17.72),\n",
      " (2, 0.03),\n",
      " (3, 0.03),\n",
      " (4, 0.03),\n",
      " (5, 0.03),\n",
      " (6, 0.03),\n",
      " (7, 33.04)]\n"
     ]
    }
   ],
   "source": [
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
