{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-28 20:45:12,350] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import deepspeed\n",
    "import vllm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "import GPUtil\n",
    "from termcolor import colored\n",
    "@register_line_cell_magic\n",
    "def vram(line, cell=None):\n",
    "    \"monitor the usage of vram\"\n",
    "    if cell:\n",
    "        get_ipython().run_cell(cell)\n",
    "    if line:\n",
    "        get_ipython().run_cell(line)\n",
    "    print(colored(\n",
    "        \"| \"+\" | \".join([f\"{i} @ {gpu.memoryUtil*100:.2f}%\" for i, gpu in enumerate(GPUtil.getGPUs())]) + \" |\", \n",
    "        \"green\"\n",
    "    ))\n",
    "    # logger.debug(\"  \".join([f\"{i}: {gpu.memoryUtil*100:.2f}%\" for i, gpu in enumerate(GPUtil.getGPUs())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.75% | 5 @ 7.73% | 6 @ 7.75% | 7 @ 7.76% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5,7\"\n",
    "os.environ['WORLD_SIZE'] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data/samsum\")\n",
    "MODEL_PATH = Path(\"../models/gpt2/base/\")\n",
    "WORK_DIR = Path('results/samsum/gpt2-base-kd')\n",
    "TEACHER_MODEL_PATH = Path(\"./results/samsum/gpt2-xlarge-sft/checkpoint-4600/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-28 20:45:12.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[34m\u001b[1mDatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2023-11-28 20:45:12.940\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[34m\u001b[1mTrain data example:\n",
      "[INST] <<SYS>>\n",
      "Use the Input to provide a summary of a conversation.\n",
      "<</SYS>>\n",
      "\n",
      "Input:\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "Summary:\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-28 20:45:13.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mThe padding token id is 50256\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(str(DATA_PATH))\n",
    "logger.debug(dataset)\n",
    "\n",
    "prompt_template = \"\"\"[INST] <<SYS>>\n",
    "Use the Input to provide a summary of a conversation.\n",
    "<</SYS>>\n",
    "\n",
    "Input:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\"\"\"\n",
    "\n",
    "logger.debug(\"Train data example:\\n\" + prompt_template.format(**dataset['train'][0]))\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "logger.debug(f\"The padding token id is {tokenizer.pad_token_id}\")\n",
    "\n",
    "CUTOFF_LEN = 512\n",
    "LABEL_SPLIT = \"Summary:\\n\"\n",
    "\n",
    "def generate_and_tokenize_prompt(instance, is_test=False):\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=CUTOFF_LEN,\n",
    "            padding=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        if(\n",
    "            result['input_ids'][-1] != tokenizer.eos_token_id\n",
    "            and len(result['input_ids']) < CUTOFF_LEN\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result['input_ids'].append(tokenizer.eos_token_id)\n",
    "            result['attention_mask'].append(1)\n",
    "        result['labels'] = result['input_ids'].copy()\n",
    "        return result\n",
    "    tokenized_full_prompt = tokenize(prompt_template.format(**instance))\n",
    "    tokenized_user_prompt = tokenize(prompt_template.format(**instance).split(LABEL_SPLIT)[0] + LABEL_SPLIT, add_eos_token=False)\n",
    "    user_prompt_len = len(tokenized_user_prompt['input_ids'])\n",
    "    tokenized_full_prompt['labels'] = [-100]*user_prompt_len + tokenized_full_prompt['labels'][user_prompt_len:]\n",
    "    if is_test:\n",
    "        tokenized_user_prompt['_id'] = instance['id']\n",
    "        return tokenized_user_prompt\n",
    "    \n",
    "    len_labels = len(tokenizer(instance['summary'])['input_ids'])\n",
    "    tokenized_full_prompt['is_label_complete'] = len(tokenized_full_prompt['labels'][user_prompt_len:]) >= len_labels\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yzhangjy/LLM/llm-kd/data/samsum/train/cache-06823d004bf2423d.arrow\n",
      "Loading cached processed dataset at /home/yzhangjy/LLM/llm-kd/data/samsum/train/cache-d2b62b440b3151af.arrow\n",
      "Loading cached processed dataset at /home/yzhangjy/LLM/llm-kd/data/samsum/test/cache-763244c730f8eaad.arrow\n",
      "Loading cached processed dataset at /home/yzhangjy/LLM/llm-kd/data/samsum/test/cache-dfdf889bd99c1a61.arrow\n"
     ]
    }
   ],
   "source": [
    "columns = ['input_ids', 'attention_mask', 'labels']\n",
    "\n",
    "train_data = dataset['train'].map(generate_and_tokenize_prompt, num_proc=1) \\\n",
    "                             .filter(lambda instance: instance['is_label_complete']) \\\n",
    "                             .select_columns(columns) \\\n",
    "                             .with_format(type='torch')\n",
    "                           \n",
    "val_data = dataset['test'].map(generate_and_tokenize_prompt, num_proc=1) \\\n",
    "                          .filter(lambda instance: instance['is_label_complete']) \\\n",
    "                          .select_columns(columns) \\\n",
    "                          .with_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-28 20:45:13.150\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[34m\u001b[1mTraining data usage: 97.2305%\u001b[0m\n",
      "\u001b[32m2023-11-28 20:45:13.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[34m\u001b[1mValidation data usage: 96.6993%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.debug(f\"Training data usage: {train_data.num_rows / dataset['train'].num_rows * 100:.4f}%\")\n",
    "logger.debug(f\"Validation data usage: {val_data.num_rows / dataset['validation'].num_rows * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_summary = dataset['train'].map(lambda x: tokenizer(x['summary'])).remove_columns(dataset['train'].column_names)max([len(ids) for ids in tokenized_summary['input_ids']])\n",
    "# max([len(ids) for ids in tokenized_summary['input_ids']])\n",
    "# label_lens = [torch.where(lab==-100, 0, 1).sum().item for lab in tokenized_dataset['train']['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.75% | 5 @ 7.73% | 6 @ 7.75% | 7 @ 7.76% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             load_in_8bit=False,\n",
    "                                             use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.75% | 5 @ 12.09% | 6 @ 7.75% | 7 @ 7.76% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "model.cuda(5)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 12.10% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(TEACHER_MODEL_PATH, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             load_in_8bit=False,\n",
    "                                             use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 24.70% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "teacher_model.cuda(5)\n",
    "teacher_model.eval()\n",
    "teacher_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    return_tensors=\"pt\",\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# %%\n",
    "dataloader = DataLoader(train_data, \n",
    "                        collate_fn=data_collator, \n",
    "                        batch_size=16)#, pin_memory=True, pin_memory_device=\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 24.70% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "data = next(dataloader._get_iterator())\n",
    "\n",
    "for k, v in data.items():\n",
    "    data[k] = v.cuda(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 352])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not train on input\n",
    "# unsqueeze for the convenience of later computations\n",
    "loss_mask = torch.where(data['labels']==-100, 0, 1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 24.70% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 47.09% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "output = model.forward(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 51.49% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "probs = F.softmax(output.logits, dim=-1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 13.74% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 58.09% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "with torch.no_grad():\n",
    "    output_teacher = teacher_model.forward(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 17.01% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 51.49% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 17.01% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 55.89% | 6 @ 8.79% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "probs_teacher = F.softmax(output_teacher['logits'], dim=-1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf_mask = torch.isinf(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kd_loss`:\n",
    "1. compute KL divergence pointwisely  (`reduction=\"none\"`)\n",
    "2. mask the input part  (`*mask`)\n",
    "3. keep only output tokens and compute batchmean on them:\n",
    "    - mean on output tokens (`/ mask.sum(1, keepdim=True)`)\n",
    "    - batchmean (mathematically correct) on the batch (`.view(len(input),-1).sum(-1).mean()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_loss = lambda input, target, mask: (F.kl_div(input.log()*mask, target*mask, reduction=\"none\") / mask.sum(1, keepdim=True)).view(len(input),-1).sum(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 17.01% | 1 @ 26.67% | 2 @ 15.98% | 3 @ 7.74% | 4 @ 7.76% | 5 @ 77.87% | 6 @ 8.80% | 7 @ 7.77% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "# loss_kd = F.kl_div(probs.log(), probs_teacher, reduction=\"none\")\n",
    "loss_kd = kd_loss(probs, probs_teacher, loss_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7298, device='cuda:5', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 17.01% | 1 @ 26.67% | 2 @ 15.99% | 3 @ 7.75% | 4 @ 7.76% | 5 @ 60.29% | 6 @ 8.80% | 7 @ 8.82% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 17.53% | 1 @ 26.67% | 2 @ 15.99% | 3 @ 8.79% | 4 @ 7.76% | 5 @ 69.12% | 6 @ 8.80% | 7 @ 8.82% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "loss_kd.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 17.53% | 1 @ 26.67% | 2 @ 15.99% | 3 @ 8.79% | 4 @ 7.76% | 5 @ 69.12% | 6 @ 8.80% | 7 @ 8.82% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 17.53% | 1 @ 26.67% | 2 @ 15.99% | 3 @ 8.79% | 4 @ 7.76% | 5 @ 69.12% | 6 @ 8.80% | 7 @ 8.82% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m| 0 @ 17.53% | 1 @ 26.67% | 2 @ 15.99% | 3 @ 8.79% | 4 @ 7.76% | 5 @ 47.87% | 6 @ 8.80% | 7 @ 8.82% |\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%vram\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
